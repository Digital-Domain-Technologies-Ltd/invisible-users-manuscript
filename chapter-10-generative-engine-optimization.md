\newpage

# Chapter 10 - Generative Engine Optimization

Discovery patterns that work for search engines and AI agents.

## Introduction

For years, developers treated "bots" as a nuisance to be blocked or a search engine crawler to be fed keywords. In 2026, that distinction has collapsed. When a customer sends their AI agent to your site to "find the best price and book a flight," that agent *is* the customer.

But there's a deeper convergence happening - one that transforms how we think about discovery itself.

Traditional SEO (Search Engine Optimisation) focused on ranking: getting your site to appear first in search results. The goal was clicks. The more visible your result, the more traffic you captured.

GEO (Generative Engine Optimisation) focuses on citation: being referenced accurately in AI-generated answers. The goal is authority. When ChatGPT or Claude answers a question about your product or service, you want them to cite you correctly - and include a path for the user to transact.

**Here's the convergence that changes everything:** The patterns that help AI agents cite you correctly are the same patterns that improve your traditional search ranking.

Structured data. Semantic HTML. Clear information architecture. Explicit metadata. These aren't separate strategies for different platforms. They're unified principles that work across the entire discovery ecosystem - Google Search, Bing, ChatGPT, Claude, Perplexity, and every other system that needs to understand what your site offers.

When you optimise for GEO, you're also optimising for SEO. When you make your site legible to agents, you're making it legible to search engines. The work compounds.

This chapter shows you how. The patterns described here are implemented on this book's own product page at <https://allabout.network/invisible-users> - a working example of GEO principles in practice.

## Death of the Click

Traditional search followed a simple flow:

1. User searches for "AI agent web design book"
2. Google shows ranked results
3. User clicks a promising link
4. User browses the site, compares options, makes a decision

The click was the currency. Search optimisation meant maximising clicks. Revenue followed from converting that traffic.

AI answer engines break this model.

1. User asks ChatGPT "What books explain how AI agents interact with websites?"
2. ChatGPT generates an answer: "The Invisible Users by Tom Cranstoun examines how websites fail AI agents and provides practical patterns for..."
3. User either accepts the recommendation or asks follow-up questions
4. The user may never visit the original sources

**The click disappears. The citation replaces it.**

If ChatGPT cites your product accurately - with the correct model name, correct specifications, correct price range - you've achieved visibility. If it sends the user to your site with a clear transaction path, you've captured value.

If ChatGPT misrepresents your product - wrong name, outdated specs, incorrect price - you lose the transaction without ever knowing it happened. The user doesn't visit your site. They don't click your ad. They simply buy from a competitor who was cited correctly.

**GEO is about controlling accuracy in a world where you can't rely on clicks to measure impact.**

## The Markdown Converter Problem

Between 2022 and 2024, a common pattern emerged: convert websites to markdown for AI chatbots. Tools like converturltomd.com stripped HTML decoration, leaving plain text that language models could process efficiently. This worked well when the primary use case was answering questions in a chat interface.

The market changed. By 2026, AI agents don't just chat - they discover, evaluate, compare, and cite sources. They need to know who published the content, when it was updated, how products are priced, what formats are available. They need structured signals to separate authoritative sources from user-generated content, to distinguish product specifications from marketing copy, to cite accurately rather than hallucinate details.

Markdown converters remove exactly these signals. When you strip HTML to markdown, you lose:

- **JSON-LD structured data** - Product details, pricing, availability, reviews
- **HTML meta tags** - Publication dates, author information, content freshness
- **Schema.org markup** - Explicit signals about content type and relationships
- **Semantic HTML attributes** - data-price, data-formats, data-isbn that make extraction reliable

A person data field becomes indistinguishable from body text. A product price loses its currency and validity period. An author attribution disappears entirely. The agent can still read your content, but it has no way to cite you accurately or prove your existence as the authoritative source.

This is why designing for both matters. Keep the clean semantic HTML structure. Preserve the metadata layers. Provide explicit signals. If an AI platform needs markdown, let them convert it themselves - but give them the rich source material to convert from, not a pre-stripped version that has already lost context.

The three layers of discovery described below represent what markdown converters remove. Implementing these layers means agents can extract accurate information whether they process full HTML or need to convert to simpler formats.

## Three Layers of Discovery

Both traditional search engines and AI agents need three types of information to understand and cite your site accurately. Think of these as concentric circles of detail, from broad site-level guidance to specific content structure.

### Layer 1 - Site-Wide Guidance

These files tell crawlers and agents what your site contains, where to find information, and which areas are off-limits.

**robots.txt** - Controls search engine crawlers:

```text
User-agent: *
Allow: /products/
Allow: /blog/
Disallow: /admin/
Disallow: /api/
Sitemap: https://example.com/sitemap.xml
```

**llms.txt** - Directs AI agents to relevant resources:

```text
# Example Store

> eCommerce site selling sustainable outdoor equipment

## Products
https://example.com/products/

## Sizing Guide
https://example.com/sizing

## Returns Policy
https://example.com/returns
```

**Why both matter:** robots.txt has been standard since 1994. Search engines respect it. llms.txt is emerging (2024-onwards) as the equivalent for AI agents. Both serve the same purpose: explicit guidance about site structure and access controls.

**The convergence:** Sites with complete robots.txt files often find llms.txt easy to create because the mental model is identical. You're documenting your site structure for machines that can't intuit context from visual design.

**Extended llms.txt with metadata (proposed pattern):** The standard llms.txt format specifies only URLs to curated content. However, when agents access llms.txt directly (bypassing HTML pages), they miss all the metadata layers described earlier - author information, company details, publication context.

This book proposes extending llms.txt with markdown-formatted metadata at the top of the file:

```markdown
# About This Site

**Author:** Tom Cranstoun
**Company:** Digital Domain Technologies
**Focus:** AI agent compatibility, web accessibility, GEO patterns
**Contact:** tom.cranstoun@gmail.com

Tom Cranstoun works on making websites accessible to both humans and AI agents. This site demonstrates the patterns described in "The Invisible Users" book.

# Curated Resources

## Book
https://allabout.network/invisible-users

## Technical Documentation
https://allabout.network/docs/agent-patterns
```

This approach compensates for metadata loss when agents read llms.txt instead of parsing full HTML pages. The markdown format provides human-readable context whilst remaining machine-parseable. Standard llms.txt parsers that expect only URLs will skip the markdown header and process the URL sections normally.

**Status:** This is a proposed enhancement, not part of the current llms.txt specification. The standard format (URL-only) remains valid, but extended metadata improves agent citation accuracy when HTML metadata is unavailable.

See Appendix H for complete llms.txt implementation guidance, including both standard URL-only format and extended markdown metadata format.

### Layer 2 - Page-Level Metadata

Once a crawler or agent reaches a specific page, metadata explains what that page contains and how it relates to the broader site.

**Meta tags** - Traditional SEO metadata:

```html
<meta name="description" content="The Invisible Users - practical guide to designing websites that work for AI agents and humans">
<meta name="keywords" content="AI agents, web design, accessibility, GEO, agent compatibility">
<meta property="og:title" content="The Invisible Users - AI Agent Web Design">
<meta property="og:description" content="How to build websites that work for both AI agents and human users">
```

**JSON-LD structured data** - Machine-readable facts:

```html
<script type="application/ld+json">
{
  "@context": "https://schema.org/",
  "@type": "Book",
  "name": "The Invisible Users",
  "description": "Designing the Web for AI Agents and Everyone Else",
  "author": {
    "@type": "Person",
    "name": "Tom Cranstoun"
  },
  "offers": {
    "@type": "Offer",
    "price": "24.99",
    "priceCurrency": "GBP",
    "availability": "https://schema.org/InStock",
    "url": "https://allabout.network/invisible-users"
  },
  "datePublished": "2026-03-31"
}
</script>
```

**The convergence:** Google's search engine has used structured data for rich snippets since 2009. The same JSON-LD that helps Google show star ratings and prices in search results helps ChatGPT cite your product accurately in answers.

You're not duplicating work. You're extending existing SEO practices to benefit a new discovery channel.

### Layer 3 - Content Structure

The deepest layer is how you structure the content itself. This is where semantic HTML and clear information architecture become critical.

**Semantic HTML** - Explicit structure:

```html
<article>
  <header>
    <h1>The Invisible Users Review</h1>
    <time datetime="2026-03-31">31 March 2026</time>
  </header>

  <section aria-label="Book Details">
    <h2>Details</h2>
    <dl>
      <dt>Pages</dt>
      <dd>320 pages</dd>
      <dt>Format</dt>
      <dd>Paperback and Digital</dd>
      <dt>Target Audience</dt>
      <dd>Developers, CTOs, Product Teams</dd>
    </dl>
  </section>

  <section aria-label="Content Overview">
    <h2>Overview</h2>
    <p>The book examines how modern web interfaces fail AI agents and provides practical patterns for building sites that work for both automated and human users...</p>
  </section>
</article>
```

**Why structure matters:** Search engines use semantic HTML to understand content hierarchy. AI agents use the same structure to extract specific facts accurately.

When an agent needs to answer "How many pages is The Invisible Users?", semantic structure makes that information findable. Without structure, the agent must infer context from surrounding text - error-prone and unreliable.

**The convergence:** Accessibility guidelines have advocated semantic HTML for decades. Search engines reward it with better content understanding. Now agents depend on it for accurate extraction.

The pattern that helps blind users navigate with screen readers also helps agents extract facts for citations.

**Designing for both audiences simultaneously:** An example book product page demonstrates how to serve both humans and AI agents with a single implementation. The page includes a visible notice stating it's an example (not a real sales page) and provides contact information. This notice appears in three forms:

1. **Visible HTML** - Yellow notice box at the top of the page, readable by all visitors and AI agents in served HTML
2. **JavaScript popup** - Interactive dialog when humans click "Add to Cart", providing additional confirmation
3. **Agent metadata section** - Structured data with page type and contact details for agents that parse hidden metadata

The same information reaches three audiences (sighted humans, AI agents, assistive technologies) through progressive enhancement. The core message exists in HTML. JavaScript adds interactivity where supported. The agent metadata provides structured extraction. No audience is excluded.

See [book-product-page.html](web/book-product-page.html) for the complete example implementation showing visible notices, popup dialogs, and agent-readable metadata working together.

**Complete working example:** The book's product page at <https://allabout.network/invisible-users> demonstrates all three discovery layers - robots.txt guidance, complete Schema.org Book + Product markup with breadcrumb navigation, and semantic HTML structure with explicit state attributes. View the page source to see how meta tags, JSON-LD structured data, and semantic content structure work together in a production implementation.

### Why Discovery Comes First

Agent-mediated commerce depends entirely on successful discovery. No matter how well-structured your checkout flow, how clear your product specifications, or how robust your transaction handling, these capabilities remain invisible if AI platforms don't cite you.

When someone asks ChatGPT "find me wireless headphones under £100," the agent researches products, compares options, and recommends specific items. If your products aren't discoverable through structured data, semantic HTML, and clear metadata, you don't appear in that research phase. The user never knows you exist.

The platforms launching agent commerce systems (Amazon's Alexa.com, Microsoft's Copilot Checkout, Google's Universal Commerce Protocol) all assume merchant discoverability. The protocols define how agents transact once they've found you, but they don't solve the discovery problem. That's your responsibility.

This is why GEO and SEO convergence matters commercially. The patterns that help AI agents cite you correctly—Schema.org JSON-LD, semantic HTML, clear information architecture—are the same patterns that improve traditional search rankings. When you optimise for GEO, SEO benefits simultaneously. The work compounds across both discovery channels.

Participation in agent-based e-commerce becomes viable only after your business has been discovered and recognised. Until then, agent-mediated transactions cannot occur.

## The Business Model Crisis - Adapt or Disappear

In November 2024, Tailwind CSS faced a business model crisis that demonstrates what happens when companies fail to adapt to agent-mediated commerce. A developer opened a pull request proposing an llms.txt endpoint for Tailwind's documentation site. The PR sat for nearly two months whilst comments piled up asking why it hadn't been merged.

On 16 January 2025, after laying off 75% of his team, Adam Wathan closed the PR with an explanation that cut to the heart of the crisis: "Making it easier for LLMs to read our docs just means less traffic to our docs which means less people learning about our paid products and the business being even less sustainable."

Tailwind's revenue model depended on documentation traffic leading to paid product discovery. When AI agents started answering developer questions without sending traffic to the docs site, the business model collapsed. The company couldn't pay salaries because their only monetisation path - traffic-driven product awareness - no longer worked in a world where agents extract information without clicking through.

**This is not unique to Tailwind.** Any business that depends on traffic-driven advertising revenue, affiliate commissions from click-throughs, or "awareness" marketing that requires site visits faces the same crisis. The agent extracts your content, answers the user's question, and the transaction happens elsewhere - or doesn't happen at all because the agent couldn't find your paid offerings.

**The adaptation required:** Transform from traffic-dependent monetisation to agent-compatible revenue models:

- **Direct transactions** - Agents can complete purchases, bookings, subscriptions directly through your site when patterns are clear
- **Paid services** - Professional consulting, implementation support, enterprise features that agents can discover and recommend
- **API access** - Agents can integrate your service programmatically if you provide clear API documentation
- **Subscriptions** - Recurring revenue models where agents help users discover value, then facilitate sign-up
- **Open source support** - Sponsorship models where agents can identify projects worth supporting and facilitate contributions

**What doesn't work in the agent era:**

- Ad-funded content sites where revenue depends on pageviews
- Affiliate marketing that relies on click-through tracking
- "Freemium" models where free docs drive awareness of paid products through browsing
- Marketing sites designed to maximise time-on-site and multiple page visits

**The JSON-LD and structured data requirements aren't optional extras.** They're survival requirements. When your documentation, products, and services are properly marked up with Schema.org data, agents can discover your paid offerings, understand pricing models, identify subscription options, and facilitate transactions. Without structured data, agents extract your free content whilst your revenue opportunities remain invisible.

**This applies beyond software:** Services accepting bookings, professionals offering consultations, subscription services, online courses, paid tools - any business must make paid offerings discoverable to agents or risk being cited for free information whilst monetisation opportunities go unnoticed.

The choice is stark: implement agent-friendly patterns including transaction-ready Schema.org markup, or watch revenue collapse whilst agents extract knowledge without driving business value. Tailwind's layoffs demonstrate this isn't theoretical - companies are already failing because their business models assume human browsing behaviour that no longer dominates discovery.

## Business Decision Framework for Discovery Optimisation

Not every business benefits equally from GEO, but every business must evaluate whether their monetisation model survives in an agent-mediated world. Some business models thrive on agent citations. Others lose revenue when agents extract content without driving traffic.

### Transaction-Focused Businesses Focus on Discoverability

If your revenue comes from completed transactions - purchases, bookings, subscriptions, service requests - agent discovery accelerates your sales cycle.

**Examples:**

- **eCommerce:** Product sales (Amazon, speciality retailers)
- **Local services:** Restaurant bookings, hotel reservations, appointment scheduling
- **B2B services:** Software demos, consultation requests, pricing inquiries
- **Subscriptions:** SaaS products, membership sites, subscription boxes

**Why it works:** Agents can complete transactions on your behalf. When ChatGPT cites your product and provides a purchase link, the user can transact without extensive research. Your transaction rate increases even as click-through rates decline.

**Strategic priority:** Complete structured data, clear transaction paths, explicit pricing, Schema.org markup for all products/services.

**Real example:** Technical publishers maintain extensive JSON-LD markup across their catalogues. This wasn't built for agents - it predates widespread agent adoption. But it creates competitive advantage as agents enter commerce. When an agent searches for "book about AI agent web design," structured data makes accurate citations trivial.

### Ad-Funded Content and Strategic Protection

If your revenue comes from advertising displayed alongside content - recipe sites, tutorial blogs, news publications - agent extraction threatens your business model.

**The dilemma:** Structured data that helps agents cite your recipes accurately also enables them to extract the recipe without sending traffic to your site. The user gets the answer. You lose the ad impression.

**Examples:**

- Recipe sites marking up ingredients and instructions with Schema.org RecipeSchema
- Tutorial sites with step-by-step how-to content
- News publishers with factual reporting that answers direct queries

**Strategic options:**

1. **Partial disclosure:** Mark up only enough content to appear in citations (recipe title, summary, rating), but require a site visit for full instructions
2. **Paywalled premium content:** Let agents cite free content, reserve detailed analysis for subscribers
3. **Value-added context:** Provide the facts agents need, but keep commentary, analysis, and community discussion on-site

**Real example:** Major recipe sites face this exact tension. Schema.org RecipeSchema improves search rankings substantially - Google rewards structured recipe data with rich snippets and featured placement. But the same markup enables ChatGPT to extract the complete recipe without driving traffic to the ad-supported site.

The solution isn't to avoid structured data. It's to mark up strategically, balancing discovery with revenue protection.

### Reference and Authority Content for Maximum Discoverability

If your business model depends on establishing authority - consulting firms, professional services, educational institutions, industry publications - agent citations build reputation.

**Why it works:** When AI agents cite your analysis, research, or expertise repeatedly, you become the authoritative source. Users seeking deeper engagement will find their way to your site, your services, or your team.

**Examples:**

- Consulting firms publishing industry analysis
- Professional services firms explaining complex regulations
- Educational institutions providing course information
- Industry publications documenting best practices

**Strategic priority:** Maximum structured data, complete llms.txt, clear authorship attribution, Schema.org markup for articles, analyses, and expert profiles.

**The long game:** Agent citations compound. The more frequently ChatGPT cites your firm's tax analysis, the more your brand becomes associated with tax expertise. When a user needs actual tax services, your authority translates to commercial opportunity.

## Measuring GEO Success Beyond Clicks

Traditional SEO metrics - rankings, clicks, impressions, click-through rates - don't directly apply to GEO. Agents don't generate "impressions." They don't "click through." They extract information and either cite you or don't.

**New metrics to track:**

### 1. Citation Frequency

How often do AI agents reference your content when answering relevant queries?

**How to measure:** Periodically test queries related to your products, services, or expertise with ChatGPT, Claude, Perplexity, and other major platforms. Track:

- Percentage of queries where your site is cited
- Position of your citation (first source, secondary source, etc.)
- Accuracy of the citation (correct facts, correct links, correct attribution)

**Target:** Aim for citation in 70%+ of highly relevant queries. If you've published a book about AI agents and ChatGPT rarely mentions it when asked about agent-compatible web design, your structured data needs work.

### 2. Traffic Source Attribution

Are users arriving from agent interactions?

**How to measure:** Check analytics for referral patterns:

- Direct traffic with no referrer (users typing URLs cited by agents)
- Traffic from new domains as agents launch their own web interfaces
- UTM parameters if you can convince platforms to include them in agent-generated links

**Limitation:** This is imperfect. Users often copy-paste links from agents, losing attribution. But trends matter more than precise counts.

### 3. Structured Data Validation

Is your structured data correct, complete, and up-to-date?

**How to measure:**

- Google's Rich Results Test (<https://search.google.com/test/rich-results>)
- Schema.org validator (<https://validator.schema.org/>)
- Structured data coverage in Google Search Console

**Target:** Zero errors, complete coverage across all product pages, services, and key content.

### 4. Agent Conversion Rate

When agents do send users to your site, do those users convert?

**How to measure:** Track conversion rates for traffic that arrives via agent citations separately from organic search traffic.

**Hypothesis:** Agent-referred users should convert at higher rates because the agent pre-qualified them. If a user asks "What book explains AI agent web compatibility?" and the agent cites your book, that user arrives ready to purchase - assuming the agent cited correctly.

If conversion rates are lower than expected, investigate whether agents are:

- Citing incorrect products
- Setting wrong expectations about price or features
- Linking to unhelpful landing pages

### 5. Combined Discovery Metrics

The real win is when GEO improvements also boost traditional SEO performance.

**What to track:**

- Organic search traffic growth alongside agent citations
- Featured snippet appearances (Google rewards structured data)
- Rich result eligibility (products, recipes, events, FAQs)
- "People also ask" inclusion (semantically structured content appears here)

**The convergence payoff:** If your JSON-LD improvements lead to both increased agent citations AND improved search rankings, you've demonstrated the core GEO thesis: optimising for one platform improves performance across the entire discovery ecosystem.

## Implementation Priorities

Where should you start? The answer depends on your current state and available resources, but the sequence below works for most businesses.

### Priority 1 - robots.txt and llms.txt (Immediate - Zero Friction)

These files require minimal effort and provide foundational guidance to all discovery systems.

**Time investment:** 30 minutes to 2 hours
**Impact:** Immediate improvement in how agents understand your site structure

**What to do:**

1. Ensure your robots.txt file is complete and current
2. Create llms.txt following the patterns in Appendix H
3. Test both files are accessible at standard locations

**Why this matters:** Sites without llms.txt force agents to guess what content is relevant. Sites with clear, complete llms.txt files guide agents directly to valuable resources.

**Real-world validation:** The implementation patterns in this chapter are demonstrated on this book's own product page. View source at <https://allabout.network/invisible-users> to see Schema.org Book + Product markup, breadcrumb navigation, semantic HTML structure, and agent-readable metadata sections in practice.

### Priority 2 - Audit Existing Schema.org Implementation

You likely already have structured data if you've done SEO work. The question is whether it's complete, accurate, and current.

**Time investment:** 2-4 hours for audit, additional time for fixes
**Impact:** Fixes errors that cause both search engines and agents to misunderstand your content

**What to do:**

1. Run your key pages through Google's Rich Results Test
2. Check Schema.org validator for errors and warnings
3. Review Google Search Console for structured data coverage issues
4. Prioritise fixes for product pages, service descriptions, and high-traffic content

**Common issues:**

- Missing required fields (price, availability, currency)
- Outdated information (old prices, expired events, incorrect stock status)
- Incomplete coverage (only some products have markup)
- Syntax errors (malformed JSON, incorrect property names)

### Priority 3 - Expand Metadata Coverage

Once your existing structured data is clean, expand coverage to additional content types.

**Time investment:** Ongoing, depends on site size
**Impact:** Broader agent coverage, more citation opportunities

**What to add:**

- FAQ markup (helps agents answer common questions)
- HowTo markup (tutorial and guide content)
- Article markup (blog posts, news, analysis)
- LocalBusiness markup (if relevant)
- Organisation and Person markup (about pages, author profiles)

**Strategic focus:** Prioritise content types where agent citations create business value. If you publish industry analysis, prioritise Article markup. If you run a local business, prioritise LocalBusiness and location data.

### Priority 4 - Improve Content Structure

This is the longest-term investment, but yields benefits across accessibility, SEO, and GEO simultaneously.

**Time investment:** Weeks to months, depending on site complexity
**Impact:** Fundamental improvement in how all machine systems understand your content

**What to improve:**

- Convert `<div>` layouts to semantic elements (`<article>`, `<section>`, `<nav>`, etc.)
- Add proper heading hierarchy (H1 → H2 → H3, no skips)
- Structure lists with `<ul>`, `<ol>`, `<dl>` instead of custom markup
- Use `<time>` elements with machine-readable dates
- Implement tables with proper headers for tabular data
- Add ARIA labels where semantic elements need additional context

**Why this is last:** Content structure improvements require more development time and coordination with design teams. But once implemented, they compound indefinitely. Every new page built with semantic structure benefits all three audiences: humans, search engines, and agents.

### Skip Links - Navigation for All Users

Skip links are a perfect example of the convergence principle: a pattern designed for keyboard and assistive technology users that also helps AI agents understand page structure.

**Understanding assistive technology users:** People with disabilities use technology in diverse ways. People with hand tremors who cannot grip a mouse use keyboards to navigate. People with mobility disabilities control computers with voice commands, eye tracking, or switch devices. People who are blind use screen readers, braille displays, or speech recognition software. People with low vision use screen magnification.

The term assistive technologies encompasses all these tools. As developers, we don't need to code for each specific technology. We use standard methods to ensure our solutions are accessible for all. When we implement semantic HTML, ARIA attributes, and clear information architecture, we're building interfaces that work across this entire ecosystem of assistive technologies - and for AI agents as well.

**What skip links are:** The first interactive element on a page, typically hidden visually but available to keyboard users and screen readers. When activated, skip links take the user directly to the main content, bypassing global navigation, headers, and repeated elements.

**How users interact with them:** When someone navigates with a keyboard (pressing Tab), the skip link receives focus first. Pressing Enter follows the link, moving focus to the main content area. This saves keyboard users from tabbing through dozens of navigation items on every page.

**HTML implementation:**

```html
<body>
  <a href="#main" class="skip">Skip to main content</a>

  <header>
    <nav>
      <!-- Global navigation with many links -->
    </nav>
  </header>

  <main id="main" tabindex="-1">
    <!-- Main content starts here -->
  </main>
</body>
```

**CSS implementation:**

```css
.skip {
  position: absolute;
  left: -10000px;
  top: auto;
  width: 1px;
  height: 1px;
  overflow: hidden;
}

.skip:focus {
  position: static;
  width: auto;
  height: auto;
}
```

The CSS hides the skip link off-screen by default, but when it receives keyboard focus, it becomes visible and positioned normally in the flow.

**Benefits across audiences:**

- **Keyboard users:** Bypass repeated navigation, reach content immediately
- **Screen reader users:** Announced as first element, provides explicit navigation choice
- **Switch controls and other assistive technologies:** Reduces interaction required to reach main content
- **AI agents:** The explicit `#main` anchor and `<main>` landmark make page structure unambiguous

**Important note:** Complex sites may benefit from multiple skip links, not just to main content. Consider skip links to search functionality, footer information, or other major page sections. Each skip link should be clearly labelled and link to a specific, focusable target.

People using keyboard navigation, screen readers, switch controls and other assistive technologies rely on skip links to reach main content and other important sections easier and faster. This method is very helpful and important for users that depend on keyboard and similar input.

## Common Implementation Mistakes

The patterns above work when implemented correctly. Here are the mistakes that prevent them from working.

### Hiding Structured Data with display:none

**The mistake:** Adding Schema.org JSON-LD inside elements with `display: none` or positioning content off-screen to hide it from human visitors.

**Why it fails:** Google and other search engines may treat this as deceptive markup and penalize your site. The structured data should describe visible content, not add information that isn't present on the page.

**The fix:** Ensure all structured data corresponds to content actually displayed on the page. If you mark up a product price of £24.99, that price should be visible to human visitors. If you mark up a 5-star rating, those stars should appear on the page.

### Structured Data and Visible Content Mismatch

**The mistake:** JSON-LD says the product costs £24.99. The visible HTML shows £29.99. Or the structured data lists an event on March 15th whilst the visible date says March 16th.

**Why it fails:** Agents extract structured data and cite it authoritatively. When users visit your site and find different information, trust collapses. Google also validates that structured data matches visible content.

**The fix:** Treat structured data as a single source of truth. Generate both your JSON-LD and visible HTML from the same data source. When you update a price, update both simultaneously. Consider using server-side templating that outputs both formats from one variable.

### Forgetting to Update dateModified

**The mistake:** Publishing an article in 2024, updating it substantially in 2026, but leaving `datePublished: "2024-06-15"` without adding `dateModified: "2026-01-17"`.

**Why it fails:** Agents prioritise recent information. An article from 2024 appears less relevant than one from 2026. If you've updated the content to reflect current information, agents should know that.

**The fix:** Add both `datePublished` and `dateModified` to Article markup. Update `dateModified` whenever you make substantial changes to the content. Minor typo fixes don't require updates, but factual corrections, new sections, or updated recommendations do.

### Using Relative URLs in JSON-LD

**The mistake:** Including URLs like `/products/book` or `images/cover.png` in structured data instead of absolute URLs.

**Why it fails:** When agents extract your structured data for citation or caching, relative URLs lose context. The agent doesn't know what domain to prepend. The URL becomes useless.

**The fix:** Use absolute URLs everywhere in JSON-LD. `https://example.com/products/book` not `/products/book`. This applies to product URLs, image URLs, author profile URLs, and any other link.

### Implementing Wrong Schema.org Properties

**The mistake:** Using `cost` instead of `price` for product offers. Using `writer` instead of `author` for books. Inventing custom properties like `productColour` instead of using the standard `color`.

**Why it fails:** Agents and search engines expect specific property names. Using non-standard names means your data gets ignored. The Schema.org vocabulary is extensive but precise.

**The fix:** Consult the official Schema.org documentation for each type you implement. Check the required and recommended properties. Use validators to catch incorrect property names. Don't guess - verify.

### Incomplete Required Fields

**The mistake:** Marking up a product with name and description but omitting `price`, `availability`, or `priceCurrency`. Or marking up an event without `startDate` and `location`.

**Why it fails:** Search engines require certain fields for rich results. Agents need complete information for accurate citations. Missing required fields means your markup doesn't validate and won't be used.

**The fix:** Use Google's Rich Results Test and Schema.org validator to identify missing required fields. Prioritise completing required fields over adding optional ones. A complete, simple implementation beats an incomplete complex one.

## Testing and Validation Workflow

Implementation is half the work. Verification ensures it actually works.

### Manual Testing - What Agents Actually See

Before trusting validators, verify what agents receive when they request your pages.

**Using curl to see served HTML:**

```bash
curl -A "Mozilla/5.0 (compatible; Googlebot/2.1)" https://example.com/products/book
```

This shows exactly what a crawler sees. If your product details load via JavaScript after page render, curl won't see them. Neither will many agents.

**Browser developer tools:**

1. Open DevTools (F12)
2. Navigate to Network tab
3. Load your page
4. Find the initial HTML document request
5. Check Response - this is what agents see before JavaScript executes

If critical information appears only after JavaScript runs, agents miss it.

**View source vs inspect element:**

- View Source (Ctrl+U) shows served HTML - what agents receive
- Inspect Element shows rendered HTML - what browsers display after JavaScript

The difference matters. Agents see View Source.

### Systematic Testing Routine

**Weekly validation:**

1. Run 5-10 key pages through Google Rich Results Test
2. Check Schema.org validator for warnings
3. Review Google Search Console for new structured data errors
4. Test one key query with ChatGPT, Claude, or Perplexity - are you cited correctly?

**Monthly audit:**

1. Full site crawl checking for structured data coverage
2. Validate that all product pages have complete markup
3. Check that recently updated content has current `dateModified`
4. Test citation accuracy across multiple AI platforms

**After any CMS update or template change:**

1. Re-validate all structured data
2. Verify no fields were accidentally removed
3. Check that template changes didn't break JSON-LD syntax
4. Test a sample of pages to ensure markup still generates correctly

### Setting Up Monitoring

**Google Search Console - Track structured data coverage:**

1. Navigate to Enhancements section
2. Review Product, Recipe, FAQ, HowTo reports
3. Set up email alerts for new errors
4. Monitor valid vs invalid item counts over time

**Schema.org validator in CI/CD:**

Add automated validation to your deployment pipeline. Generate sample pages, validate their structured data, fail the build if validation errors occur. This prevents broken markup from reaching production.

**Custom monitoring script:**

```javascript
// Check critical pages daily
const criticalPages = [
  'https://example.com/products/bestseller',
  'https://example.com/about',
  'https://example.com/services'
];

criticalPages.forEach(async (url) => {
  const response = await fetch(url);
  const html = await response.text();
  const jsonLdMatches = html.match(/<script type="application\/ld\+json">(.*?)<\/script>/gs);

  if (!jsonLdMatches) {
    alert(`No JSON-LD found on ${url}`);
  }

  jsonLdMatches.forEach(match => {
    try {
      JSON.parse(match);
    } catch(e) {
      alert(`Invalid JSON-LD on ${url}: ${e.message}`);
    }
  });
});
```

Run this daily via cron job or CI/CD. Alert when structured data disappears or becomes malformed.

### Testing with Actual AI Agents

Validators tell you if syntax is correct. Agents tell you if it's useful.

**Monthly agent testing protocol:**

1. Identify 5 queries relevant to your business
2. Ask ChatGPT each query, note if you're cited and how accurately
3. Repeat with Claude, Perplexity, and Google SGE if available
4. Document citation frequency, accuracy, and link inclusion
5. Track changes month-over-month

**Example queries for a technical book:**

- "What books explain how AI agents interact with websites?"
- "Best resources for learning about GEO?"
- "Books about web accessibility and AI agents"
- "Technical guides for making websites AI-friendly"
- "Where can I learn about Schema.org for agent compatibility?"

If you're cited in 2 out of 5 queries, you have a 40% citation rate. Track this over time. Improvements to structured data should increase citation frequency and accuracy.

## Multi-Language and International Sites

GEO principles apply globally, but implementation details vary by region and language.

### Language Indication and hreflang Tags

**The challenge:** Your site exists in English, German, and Japanese. Agents need to know which version to cite for queries in each language.

**The solution:** Combine Schema.org with hreflang annotations.

```html
<link rel="alternate" hreflang="en" href="https://example.com/en/products/book" />
<link rel="alternate" hreflang="de" href="https://example.com/de/products/book" />
<link rel="alternate" hreflang="ja" href="https://example.com/ja/products/book" />
<link rel="alternate" hreflang="x-default" href="https://example.com/en/products/book" />
```

Add `inLanguage` to your JSON-LD:

```json
{
  "@type": "Book",
  "name": "The Invisible Users",
  "inLanguage": "en-GB"
}
```

**Why both matter:** hreflang tells search engines about alternate language versions. `inLanguage` tells agents and structured data consumers what language the content is written in. Both improve citation accuracy.

### Currency Handling in Product Markup

**The challenge:** You sell to UK, US, and EU customers. Prices vary by region due to VAT, shipping costs, and currency conversion. International customers need clear pricing in their local currency.

**The solution:** Use multi-regional offer arrays with `eligibleRegion` to provide region-specific pricing whilst keeping all offers in a single structured data block.

**Single product with multiple regional offers:**

```json
{
  "@type": "Product",
  "name": "The Invisible Users (eBook)",
  "offers": [
    {
      "@type": "Offer",
      "price": "24.99",
      "priceCurrency": "GBP",
      "priceValidUntil": "2026-12-31",
      "valueAddedTaxIncluded": true,
      "eligibleRegion": {
        "@type": "Place",
        "name": "United Kingdom"
      }
    },
    {
      "@type": "Offer",
      "price": "32.99",
      "priceCurrency": "USD",
      "priceValidUntil": "2026-12-31",
      "valueAddedTaxIncluded": false,
      "eligibleRegion": {
        "@type": "Place",
        "name": "United States"
      }
    },
    {
      "@type": "Offer",
      "price": "29.99",
      "priceCurrency": "EUR",
      "priceValidUntil": "2026-12-31",
      "valueAddedTaxIncluded": true,
      "eligibleRegion": {
        "@type": "Place",
        "name": "European Union"
      }
    }
  ]
}
```

**Benefits of this approach:**

- Agents can select appropriate regional pricing based on user location
- All pricing information in one place reduces maintenance burden
- Clear `eligibleRegion` eliminates ambiguity about which price applies where
- `valueAddedTaxIncluded` indicates tax treatment per region
- Search engines understand regional availability and pricing

**Alternative - Region-specific pages:**

If you maintain separate pages per region (example.com/uk/, example.com/us/), each page should only include the relevant regional offer:

**UK page (example.com/uk/product):**

```json
{
  "@type": "Product",
  "offers": {
    "@type": "Offer",
    "price": "24.99",
    "priceCurrency": "GBP",
    "priceValidUntil": "2026-12-31",
    "valueAddedTaxIncluded": true
  }
}
```

**US page (example.com/us/product):**

```json
{
  "@type": "Product",
  "offers": {
    "@type": "Offer",
    "price": "32.99",
    "priceCurrency": "USD",
    "priceValidUntil": "2026-12-31",
    "valueAddedTaxIncluded": false
  }
}
```

**Critical:** Don't show USD prices on the GBP page. Agents extract structured data as authoritative. Mismatches between currency and region reduce trust. If using region-specific pages, only include the pricing for that region. If using a single international page, include all regional offers in an array with explicit `eligibleRegion` properties.

### Language-Specific llms.txt Files

**The pattern:** Create regional llms.txt files that direct agents to language-specific content.

**Root llms.txt (English):**

```markdown
# Example Company

> International software company specialising in web accessibility

## English Content
https://example.com/en/products
https://example.com/en/docs
https://example.com/en/blog

## Other Languages
https://example.com/de/llms.txt (German)
https://example.com/ja/llms.txt (Japanese)
```

**German llms.txt at /de/llms.txt:**

```markdown
# Example Company (Deutsch)

> Internationales Softwareunternehmen mit Schwerpunkt Barrierefreiheit

## Deutsche Inhalte
https://example.com/de/produkte
https://example.com/de/dokumentation
https://example.com/de/blog
```

This guides agents to language-appropriate content whilst keeping the structure manageable.

### Regional Price Variations

**The challenge:** VAT treatment varies by product type and region. Digital products (eBooks, software) are standard-rated at 20% VAT in the UK. Physical products (printed books, goods) may be zero-rated (0% VAT) in some jurisdictions. US pricing typically excludes sales tax, shown at checkout. Different shipping costs apply by region.

**The solution:** Be explicit about what's included, using `valueAddedTaxIncluded` to indicate tax status clearly.

**UK eBook (VAT-inclusive at 20%):**

```json
{
  "@type": "Offer",
  "price": "24.99",
  "priceCurrency": "GBP",
  "priceSpecification": {
    "@type": "UnitPriceSpecification",
    "price": "24.99",
    "priceCurrency": "GBP",
    "valueAddedTaxIncluded": true
  }
}
```

**UK printed book (zero-rated, no VAT):**

```json
{
  "@type": "Offer",
  "price": "34.99",
  "priceCurrency": "GBP",
  "priceSpecification": {
    "@type": "UnitPriceSpecification",
    "price": "34.99",
    "priceCurrency": "GBP",
    "valueAddedTaxIncluded": false
  }
}
```

**US eBook (pre-tax):**

```json
{
  "@type": "Offer",
  "price": "32.99",
  "priceCurrency": "USD",
  "priceSpecification": {
    "@type": "UnitPriceSpecification",
    "price": "32.99",
    "priceCurrency": "USD",
    "valueAddedTaxIncluded": false
  },
  "eligibleRegion": {
    "@type": "Place",
    "name": "United States"
  }
}
```

**US printed book (pre-tax):**

```json
{
  "@type": "Offer",
  "price": "44.99",
  "priceCurrency": "USD",
  "priceSpecification": {
    "@type": "UnitPriceSpecification",
    "price": "44.99",
    "priceCurrency": "USD",
    "valueAddedTaxIncluded": false
  },
  "eligibleRegion": {
    "@type": "Place",
    "name": "United States"
  }
}
```

**Multi-regional offer array:**

When selling to multiple regions, use an array of offers with `eligibleRegion` to specify which price applies where:

```json
{
  "@type": "Product",
  "name": "The Invisible Users (eBook)",
  "offers": [
    {
      "@type": "Offer",
      "price": "24.99",
      "priceCurrency": "GBP",
      "valueAddedTaxIncluded": true,
      "eligibleRegion": {
        "@type": "Place",
        "name": "United Kingdom"
      }
    },
    {
      "@type": "Offer",
      "price": "32.99",
      "priceCurrency": "USD",
      "valueAddedTaxIncluded": false,
      "eligibleRegion": {
        "@type": "Place",
        "name": "United States"
      }
    }
  ]
}
```

Agents citing your product can accurately represent whether tax is included, preventing confusion when users discover unexpected charges at checkout. The `eligibleRegion` property helps agents select the appropriate price for their user's location.

## Dynamic Content and Technical Challenges

Modern web applications create GEO challenges that static sites don't face.

### Single Page Applications (SPAs)

**The problem:** React, Vue, and Angular apps render content client-side. The initial HTML response contains minimal content - usually just a root div and JavaScript bundles. Agents requesting the page see an empty shell.

**Solution 1 - Server-Side Rendering (SSR):**

Frameworks like Next.js (React), Nuxt.js (Vue), and Angular Universal render pages on the server before sending HTML to the client. Agents receive complete content.

**Trade-offs:** Requires architecture changes. Increases server load. Worth it for content-heavy sites where agent discoverability matters.

**Solution 2 - Static Site Generation (SSG):**

Pre-render pages at build time. Tools like Gatsby, Next.js static export, or Astro generate complete HTML for every page during deployment. No server-side rendering required.

**Trade-offs:** Works for content that doesn't change frequently. Product catalogues, documentation, marketing sites. Less suitable for personalised dashboards or real-time data.

**Solution 3 - Hybrid Rendering:**

SSR for public-facing pages (products, blog posts, documentation). Client-side rendering for authenticated areas (user dashboards, account settings). Agents only need access to public pages anyway.

**Solution 4 - Pre-rendering at the Edge:**

Services like Cloudflare Workers, Fastly, or Vercel Edge Functions can pre-render JavaScript-heavy pages specifically for agents whilst serving the SPA normally to browsers.

### Frequently Changing Content

**The challenge:** Stock levels update every few minutes. Prices change based on demand. Availability shifts throughout the day. How do you keep structured data current without regenerating pages constantly?

**Solution 1 - Cache with short TTL:**

Generate structured data server-side with a 5-10 minute cache. Agents requesting pages multiple times within that window see cached data. Fresh requests get updated information.

**Solution 2 - Separate API for structured data:**

Serve static HTML with JavaScript that fetches current prices, stock levels, and availability from an API. For agents, include structured data with:

```json
{
  "@type": "Product",
  "offers": {
    "@type": "Offer",
    "price": "24.99",
    "priceCurrency": "GBP",
    "availability": "https://schema.org/InStock",
    "priceValidUntil": "2026-01-17T23:59:59Z"
  }
}
```

Set `priceValidUntil` to the current day. Agents know the price is recent. If they cache the data, they know when it expires.

**Solution 3 - Aggregate offers:**

If prices vary frequently (airline tickets, hotel rooms), use AggregateOffer:

```json
{
  "@type": "Product",
  "offers": {
    "@type": "AggregateOffer",
    "lowPrice": "89.99",
    "highPrice": "199.99",
    "priceCurrency": "GBP",
    "offerCount": "12"
  }
}
```

This provides agents with price ranges rather than exact values that might be stale.

### User-Generated Content

**The challenge:** Product reviews, forum posts, comments - content created by users that you don't directly control.

**Schema.org Review markup:**

```json
{
  "@type": "Product",
  "name": "The Invisible Users",
  "review": [
    {
      "@type": "Review",
      "author": {
        "@type": "Person",
        "name": "Sarah Chen"
      },
      "reviewRating": {
        "@type": "Rating",
        "ratingValue": "5",
        "bestRating": "5"
      },
      "datePublished": "2026-04-15",
      "reviewBody": "Essential reading for any web team..."
    }
  ]
}
```

**Critical distinction:** Mark up actual reviews that appear on the page, not fake reviews created for SEO. Google penalises review manipulation. Agents citing fake reviews damage your reputation.

**Verification:** Add `reviewRating` only for reviews where users actually provided ratings. Add `datePublished` only if you track when reviews were written. Don't fabricate metadata to make reviews look more authoritative than they are.

### Personalised Content

**The challenge:** Showing different prices, products, or recommendations based on user location, browsing history, or account type.

**The solution:** Structured data should reflect what most visitors see, not personalised variations.

**Example:** A logged-out visitor from the UK should see:

```json
{
  "@type": "Product",
  "offers": {
    "@type": "Offer",
    "price": "24.99",
    "priceCurrency": "GBP",
    "availability": "https://schema.org/InStock"
  }
}
```

Even if logged-in enterprise customers see a different price due to volume discounts, the structured data reflects the standard retail price. This prevents agents from citing personalised pricing that doesn't apply to most users.

**For region-specific content:** Use IP geolocation or `Accept-Language` headers to serve appropriate structured data. A visitor from Germany sees EUR pricing. A visitor from the US sees USD pricing. But each sees one consistent price, not a personalised discount.

## Schema.org Type Selection Guide

Choosing the correct Schema.org type significantly impacts how agents interpret your content.

### Common Business Models and Schema.org Types

**eCommerce Product Pages:**

- Primary: `Product`
- Also consider: `Offer`, `AggregateRating`, `Review`
- Required fields: `name`, `offers` (with `price`, `priceCurrency`)

**Local Businesses:**

- Primary: `LocalBusiness` (or specific subtype like `Restaurant`, `Store`)
- Also consider: `PostalAddress`, `GeoCoordinates`, `OpeningHoursSpecification`
- Required fields: `name`, `address`, `telephone`

**Professional Services:**

- Primary: `ProfessionalService` or `Service`
- Also consider: `Offer` (for pricing), `AggregateRating`
- Required fields: `name`, `provider`, `areaServed`

**Content Publishers:**

- Primary: `Article`, `BlogPosting`, or `NewsArticle`
- Also consider: `Person` (author), `Organization` (publisher)
- Required fields: `headline`, `datePublished`, `author`

**Software and SaaS:**

- Primary: `SoftwareApplication` or `WebApplication`
- Also consider: `Offer` (for pricing), `AggregateRating`
- Required fields: `name`, `applicationCategory`, `offers`

**Educational Content:**

- Primary: `Course` or `EducationalOrganization`
- Also consider: `Person` (instructor), `Review`
- Required fields: `name`, `description`, `provider`

**Events:**

- Primary: `Event`
- Also consider: `Place` (location), `Offer` (tickets)
- Required fields: `name`, `startDate`, `location`

### When to Use Multiple Types

**Combining types expands agent understanding.**

**Book that's also a product:**

```json
{
  "@context": "https://schema.org/",
  "@type": ["Book", "Product"],
  "name": "The Invisible Users",
  "author": {
    "@type": "Person",
    "name": "Tom Cranstoun"
  },
  "isbn": "XXX-X-XXXXX-XX-X",
  "numberOfPages": 320,
  "offers": {
    "@type": "Offer",
    "price": "24.99",
    "priceCurrency": "GBP"
  }
}
```

Using both `Book` and `Product` types allows agents to extract bibliographic information (ISBN, author, page count) AND commercial information (price, availability).

**Article from a news organisation:**

```json
{
  "@type": ["NewsArticle", "Article"],
  "headline": "AI Agents Transform Web Discovery",
  "datePublished": "2026-01-17",
  "publisher": {
    "@type": "Organization",
    "name": "Tech News Daily"
  }
}
```

`NewsArticle` signals journalistic content with time sensitivity. `Article` is the broader type that covers general written content.

### Choosing Between Similar Types

**Offer vs AggregateOffer:**

Use `Offer` when you have a single, specific price:

```json
{
  "offers": {
    "@type": "Offer",
    "price": "24.99",
    "priceCurrency": "GBP"
  }
}
```

Use `AggregateOffer` when you have multiple sellers or price variations:

```json
{
  "offers": {
    "@type": "AggregateOffer",
    "lowPrice": "19.99",
    "highPrice": "24.99",
    "priceCurrency": "GBP",
    "offerCount": "5"
  }
}
```

**Article vs BlogPosting vs NewsArticle:**

- `Article` - General written content, evergreen
- `BlogPosting` - Personal or informal writing, opinion pieces
- `NewsArticle` - Journalistic reporting, time-sensitive

**LocalBusiness vs Organization:**

- `LocalBusiness` - Physical location matters (restaurants, shops, offices)
- `Organization` - Abstract entity (corporations, institutions, online-only businesses)

### Required vs Recommended Properties

Every Schema.org type has required, recommended, and optional properties.

**Required properties** - Must be present for valid markup. Missing required fields means the markup may be ignored.

**Recommended properties** - Strongly encouraged. Improves agent understanding and increases likelihood of citation.

**Optional properties** - Nice to have. Add them if the information is relevant and you have the data, but don't fabricate information just to include more properties.

**Example - Product type:**

Required:

- `name`
- `offers` (which itself requires `price`, `priceCurrency`)

Recommended:

- `image`
- `description`
- `brand`
- `review` or `aggregateRating`

Optional:

- `sku`
- `gtin`
- `mpn`
- `category`
- `color`
- `material`

**Strategy:** Get required fields correct first. Add recommended fields next. Add optional fields only if they provide meaningful information that agents might cite.

## llms.txt Evolution and Monitoring

Since llms.txt is an emerging standard, tracking its adoption and effectiveness requires active monitoring.

### Verifying Agent Usage

**The challenge:** You've published llms.txt, but are agents actually reading it?

**Solution 1 - Server logs:**

Check your web server logs for requests to `/llms.txt`. Look for user agents indicating AI platforms:

```
"Mozilla/5.0 (compatible; Claude/1.0; +https://anthropic.com)"
"GPTBot/1.0"
"PerplexityBot/1.0"
```

If you see regular requests, agents are discovering your file. If you see none, either agents aren't visiting your site or they're not looking for llms.txt yet.

**Solution 2 - Analytics:**

Add a tracking pixel or JavaScript snippet to an HTML page linked from llms.txt. When agents follow those links, you'll see traffic attributed to llms.txt referrals.

**Limitation:** Many agents won't execute JavaScript, so this undercounts actual usage. But it provides a lower bound.

**Solution 3 - Citation monitoring:**

Test queries with AI platforms monthly. Track whether citations improve after you publish or update llms.txt. Correlation suggests the file is being used.

### Content Refresh Cadence

**When to update llms.txt:**

**Immediately:**

- New major product or service launches
- Significant content additions (new documentation sections, product categories)
- Company rebranding or URL structure changes
- Deprecated content removal

**Quarterly:**

- Review all linked URLs for accuracy
- Add recently published high-value content
- Remove outdated or low-quality pages
- Update descriptions to reflect current focus

**Annually:**

- Complete audit of site structure
- Reorganise sections if your business focus has shifted
- Refresh metadata (company description, contact information)
- Review whether linked pages still represent your best content

**Version control llms.txt:** Track changes in git. Review diffs to ensure updates are intentional. Accidental deletions or malformed syntax can break agent discovery.

### Examples for Different Business Types

**SaaS Company:**

```markdown
# ProductName

> B2B project management software for distributed teams

## Product Information
https://example.com/features
https://example.com/pricing
https://example.com/security

## Documentation
https://example.com/docs
https://example.com/api-reference
https://example.com/integrations

## Resources
https://example.com/blog
https://example.com/case-studies
https://example.com/webinars
```

**Professional Services Firm:**

```markdown
# Law Firm Name

> Commercial law firm specialising in technology transactions and intellectual property

## Practice Areas
https://example.com/services/technology-law
https://example.com/services/ip-law
https://example.com/services/contract-negotiation

## Team
https://example.com/team

## Insights
https://example.com/articles
https://example.com/legal-updates
```

**Content Publisher:**

```markdown
# Publication Name

> Independent technology journalism covering AI, privacy, and digital rights

## Latest Articles
https://example.com/latest

## Topics
https://example.com/ai
https://example.com/privacy
https://example.com/policy

## About
https://example.com/about
https://example.com/ethics-guidelines
https://example.com/contact
```

**Local Business:**

```markdown
# Restaurant Name

> Italian restaurant in Manchester city centre specialising in regional cuisine

## Information
https://example.com/menu
https://example.com/reservations
https://example.com/location
https://example.com/about

## Opening Hours
Monday-Saturday: 17:00-22:00
Sunday: Closed
```

### Managing Large Sites

**The challenge:** You have hundreds or thousands of pages. Which do you include in llms.txt?

**The strategy - Curated highlights, not comprehensive listings:**

llms.txt is not a sitemap. It's a curated guide to your most important content. Aim for 10-30 URLs that represent your site's core value.

**Selection criteria:**

1. **Evergreen content** - Pages that remain relevant for months or years
2. **High authority** - Your most authoritative, comprehensive resources
3. **Clear value proposition** - Pages that clearly explain what you offer
4. **Well-structured** - Pages with good semantic HTML and structured data

**Category approach for large sites:**

```markdown
# Large eCommerce Site

> Online retailer with 10,000+ products across multiple categories

## Category Pages (navigate to specific products)
https://example.com/categories/electronics
https://example.com/categories/home-garden
https://example.com/categories/clothing

## Best Sellers
https://example.com/best-sellers

## Site Information
https://example.com/shipping
https://example.com/returns
https://example.com/about
```

Link to category pages, not individual products. Let agents navigate from categories to specific items. This keeps llms.txt manageable whilst making all products discoverable.

## Competitive GEO Analysis

Understanding how competitors approach GEO reveals opportunities and gaps.

### Auditing Competitor Implementation

**Step 1 - Check for llms.txt:**

Visit `https://competitor.com/llms.txt`. If it exists, analyse:

- How many URLs are listed?
- What content types are prioritised?
- Is there metadata or just URLs?
- How recently was it updated?

**Step 2 - Inspect structured data:**

Use Google's Rich Results Test on competitor product pages, service descriptions, and blog posts. Note:

- Which Schema.org types they use
- Completeness of required fields
- Presence of recommended properties
- Any custom or advanced markup

**Step 3 - Test agent citations:**

Query relevant terms with ChatGPT, Claude, and Perplexity. Track:

- How often competitors appear in citations
- Position (first cited, mentioned later, not mentioned)
- Accuracy of citations (correct facts, prices, dates)
- Whether agents link to competitor sites

**Step 4 - Analyse semantic structure:**

View source on competitor pages. Check for:

- Proper heading hierarchy
- Semantic HTML elements (article, section, nav)
- Explicit date markup with `<time>` elements
- Clear information architecture

### Identifying Citation Gaps

**The pattern:** Queries where competitors are cited but you're not reveal specific opportunities.

**Example queries to test:**

- "Best [product category] for [use case]"
- "[Industry] companies specialising in [service]"
- "Where to learn about [topic you cover]"
- "[Product type] with [specific feature]"
- "[Service] in [location]"

If competitors consistently appear and you don't, investigate why:

- Do they have better structured data for that content type?
- Do they have content you're missing?
- Is their llms.txt better at highlighting relevant pages?
- Do they have more authoritative backlinks or mentions?

**Closing the gap:**

1. Create content that addresses the query directly
2. Implement Schema.org markup matching or exceeding competitor completeness
3. Add relevant pages to llms.txt
4. Build authority through backlinks, citations, and mentions
5. Re-test monthly to track improvement

### Tools for Comparing Schema.org Coverage

**Manual inspection:**

Use browser extensions like "Schema.org structured data" or "Structured Data Testing Tool" to quickly view markup on any page. Visit competitor pages and your own pages side-by-side. Compare:

- Number of structured data blocks
- Types being used
- Completeness of each block
- Accuracy of data

**Automated crawling:**

Tools like Screaming Frog SEO Spider can crawl a site and extract structured data from every page. Run reports on:

- Percentage of pages with structured data
- Most common types used
- Pages missing structured data
- Validation errors

**Comparison spreadsheet:**

Create a matrix showing structured data coverage:

| Page Type | Your Site | Competitor A | Competitor B |
| --------- | --------- | ------------ | ------------ |
| Product pages | Product + Offer | Product + Offer + Review | Product only |
| Blog posts | Article | Article + Person | None |
| About page | Organization | Organization + Person | None |
| Service pages | Service | Service + Offer | None |

This reveals patterns. If competitors consistently mark up content types you're missing, prioritise adding that markup.

### Learning from Industry Leaders

**Identify best-in-class implementations:**

Within your industry, who appears most frequently in agent citations? Who has the most complete structured data? Study their approach.

**What to learn:**

- Which content types they prioritise in llms.txt
- How they structure product or service information
- Their approach to author attribution and expertise signals
- How they handle multi-language or multi-region content
- Advanced markup patterns (FAQs, HowTos, breadcrumbs)

**Adapt, don't copy:**

Take principles, not exact implementations. If a competitor successfully uses FAQ markup to capture question-based queries, implement your own FAQs addressing questions relevant to your products. Don't copy their questions verbatim.

## Cost-Benefit Analysis for GEO Investment

GEO requires time and resources. Understanding the return on that investment guides prioritisation.

### Estimating ROI

**The challenge:** Traditional marketing ROI is straightforward - spend £1000 on ads, generate £3000 in sales, ROI is 200%. GEO doesn't work that way. You can't directly attribute sales to agent citations.

**Indirect measurement:**

1. **Traffic attribution** - Set up tracking for agent-referred visitors. Monitor conversion rates for that traffic segment. If agent-referred visitors convert at 15% whilst organic search converts at 8%, agent traffic is nearly 2x more valuable.

2. **Citation value proxy** - If ChatGPT cites your product in response to 70% of relevant queries, and 1 million people ask those queries monthly, you've achieved 700,000 "citations." If even 1% click through, that's 7,000 visitors. If 10% convert at £50 average order value, that's £35,000 in revenue attributed to agent citations.

3. **SEO uplift** - Structured data improvements that enhance agent citations also improve search rankings. Track organic search traffic and rankings before and after implementing GEO patterns. The combined benefit validates the investment even if agent-specific impact is hard to isolate.

**Conservative estimate:**

If implementing complete Schema.org markup across 100 product pages takes 40 hours at £50/hour (£2,000 investment), and it drives 50 additional sales over 12 months at £30 profit margin (£1,500 return), ROI is negative. But if it also improves search rankings leading to 100 additional organic sales (£3,000), total return is £4,500 on £2,000 investment - 125% ROI.

### Prioritising Pages by Value

**High-traffic pages first:**

If 20% of your pages drive 80% of your traffic, optimise those pages first. Even small improvements to high-traffic pages compound quickly.

**High-value transactions:**

If selling enterprise software with £10,000 average contract value, optimising the 10 pages most relevant to enterprise buyers yields more business value than optimising 100 blog posts.

**Long-tail content:**

Conversely, if you publish educational content and revenue comes from affiliate links or consulting inquiries, optimising your top 50 how-to articles might drive more qualified traffic than optimising product pages.

**Matrix approach:**

| Page Type | Traffic | Value | Priority |
| --------- | ------- | ----- | -------- |
| Homepage | 10,000/mo | High | P1 |
| Best-selling products | 5,000/mo | High | P1 |
| Category pages | 3,000/mo | Medium | P2 |
| Blog posts | 2,000/mo | Low | P3 |
| About page | 500/mo | Low | P4 |

Focus on P1 pages first. Implement structured data, clean up semantic HTML, add to llms.txt. Once P1 is complete, move to P2.

### Build vs Buy - Tooling Decisions

**Manual implementation:**

**Pros:** Complete control, no recurring costs, deep understanding of your markup.

**Cons:** Time-intensive, requires technical skill, harder to maintain at scale.

**Best for:** Small sites (under 100 pages), technical teams comfortable with JSON-LD, unique content types not well-served by automated tools.

**Automated tools (Schema App, Yoast, RankMath):**

**Pros:** Fast implementation, pre-built templates, ongoing updates as Schema.org evolves, visual editors for non-technical users.

**Cons:** Recurring subscription costs, less flexibility for custom implementations, may generate bloated markup with unnecessary properties.

**Best for:** Large sites needing quick coverage, non-technical content teams, standard business models (eCommerce, local business, content publisher).

**Hybrid approach:**

Use automated tools for standard pages (products, blog posts) where templates work well. Implement custom JSON-LD manually for unique content types (complex services, multi-step processes, specialised products).

**Decision framework:**

- Under 50 pages + technical team = Manual
- 50-500 pages + standard business model = Automated tool
- 500+ pages = Automated tool with custom overrides for key pages
- Unique content types = Manual for those pages, automated for the rest

### When to Invest in Advanced Implementations

**Basic GEO** (required for all businesses):

- Complete Schema.org markup on key pages
- llms.txt with curated links
- Semantic HTML structure
- Valid JSON-LD syntax

**Intermediate GEO** (recommended for competitive industries):

- FAQ markup for common questions
- Review and rating markup
- Breadcrumb navigation markup
- HowTo markup for instructional content
- Complete metadata on all pages

**Advanced GEO** (differentiator for market leaders):

- Multi-language structured data
- Custom Schema.org types for niche industries
- Comprehensive API documentation for agents
- Real-time structured data updates
- Agent-specific content optimisation

**When to skip advanced implementations:**

If you're a local business with 10 service pages, advanced GEO won't provide proportional benefit. Basic structured data and llms.txt capture 90% of the value.

If you're an enterprise software company competing for enterprise sales, advanced GEO becomes a competitive advantage. Agents evaluating complex software need detailed structured data to make accurate recommendations.

## Agent Platform Differences

Not all AI agents behave identically. Understanding platform-specific behaviours improves citation strategies.

### ChatGPT vs Claude vs Perplexity

**ChatGPT:**

- Strong emphasis on Schema.org structured data
- Tends to cite well-known brands more frequently
- Uses training data (knowledge cutoff) plus web search for recent queries
- Respects robots.txt, adoption of llms.txt unclear as of early 2026

**Claude:**

- Processes full HTML structure, benefits from semantic markup
- Citations often include reasoning about source authority
- Respects robots.txt, actively experimenting with llms.txt support
- May prioritise .txt and .md files for quick context loading

**Perplexity:**

- Explicitly designed as a search-first answer engine
- Always provides source citations with URLs
- Heavily emphasises recency (newer content cited more often)
- Strong integration with web search APIs

**Implications for GEO:**

For ChatGPT:

- Prioritise Schema.org completeness
- Ensure brand mentions and backlinks strengthen authority signals
- Keep structured data current for recent queries

For Claude:

- Invest in semantic HTML and clear content structure
- Use llms.txt to direct attention to authoritative pages
- Consider providing text-based summaries (markdown documentation)

For Perplexity:

- Update `dateModified` regularly to signal freshness
- Focus on time-sensitive content where you have expertise
- Ensure URLs in structured data are current and accessible

### robots.txt vs llms.txt Adoption

**As of early 2026:**

**robots.txt** - Universal adoption. All major search engines and most responsible agents respect Disallow directives.

**llms.txt** - Emerging standard. Supported by some platforms, ignored by others. Early adopters include developer-focused tools and open source projects.

**Strategy:**

Implement both. robots.txt protects sensitive areas (admin panels, API endpoints, user data). llms.txt guides agents to high-value public content.

**Testing adoption:**

Check server logs for requests to `/llms.txt` from various user agents. Track which platforms are accessing the file. Adjust strategy as adoption spreads.

### Browser-Based Agents vs API-Based Agents

**Browser-based agents** (Browser Use, Claude Computer Use, Playwright-based tools):

- Execute JavaScript
- See rendered HTML after all client-side code runs
- Can interact with dynamic content
- May struggle with SPAs that require complex navigation

**API-based agents** (Agents calling your REST API directly):

- Bypass HTML entirely
- Work with structured JSON responses
- Need complete API documentation
- Can't discover functionality through visual interface

**CLI agents** (Local tools like Claude Code, Cline):

- Access served HTML only, no JavaScript execution
- Rely on explicit markup and semantic structure
- Work well with llms.txt and structured data
- May read documentation files directly (README.md, docs/)

**Design implications:**

If your business serves API-first platforms, invest heavily in API documentation, OpenAPI specs, and clear endpoint descriptions.

If your business serves browser-based agents, ensure your interface works with JavaScript disabled for initial discovery, then progressive enhancement provides full functionality.

If your business serves CLI agents, prioritise llms.txt, semantic HTML, and clear text-based documentation.

### Known Parsing Issues

**Platform-specific quirks as of early 2026:**

**Issue: Nested JSON-LD confusion**

Some agents struggle with deeply nested JSON-LD structures. Keep nesting to 3 levels maximum where possible.

**Example - Too complex:**

```json
{
  "@type": "Product",
  "offers": {
    "@type": "Offer",
    "seller": {
      "@type": "Organization",
      "address": {
        "@type": "PostalAddress",
        "addressCountry": {
          "@type": "Country",
          "name": "United Kingdom"
        }
      }
    }
  }
}
```

**Example - Simplified:**

```json
{
  "@type": "Product",
  "offers": {
    "@type": "Offer",
    "seller": {
      "@type": "Organization",
      "name": "Company Name",
      "address": {
        "@type": "PostalAddress",
        "addressCountry": "GB"
      }
    }
  }
}
```

**Issue: Currency symbol interpretation**

Some agents parse "£24.99" correctly. Others interpret the £ symbol incorrectly. Always use `priceCurrency: "GBP"` alongside numeric price values.

**Issue: Date format inconsistency**

ISO 8601 format (`2026-01-17T14:30:00Z`) is universally understood. Other date formats may be parsed inconsistently. Use ISO 8601 everywhere.

**Mitigation:**

Test your structured data with multiple validators (Google Rich Results, Schema.org validator, Bing Markup Validator). If different validators interpret your markup differently, simplify until all agree.

## Progressive Enhancement Strategy for GEO

Not every business needs advanced GEO immediately. This framework helps you implement incrementally.

### Phase 1 - Minimum Viable GEO

**Goal:** Ensure agents can discover your site and extract basic information accurately.

**Requirements:**

1. **robots.txt** - Accessible at `/robots.txt`, allows crawlers to access public pages, blocks admin/private areas
2. **llms.txt** - Published at `/llms.txt` with 5-10 curated URLs to your most important pages
3. **Basic Schema.org** - Product or Service markup on key pages with required fields only (name, description, offers with price)
4. **Semantic HTML** - Proper heading hierarchy (H1 → H2 → H3), `<article>` and `<section>` elements for main content
5. **Contact information** - Clear, machine-readable contact details (email, phone, address if applicable)

**Success criteria:** Agents can find your site, identify what you offer, and extract basic facts (product names, prices, service descriptions).

**Time to implement:** 4-8 hours for small sites, 2-3 days for medium sites.

### Phase 2 - Competitive GEO

**Goal:** Match or exceed standard industry practices for your business type.

**Requirements:**

1. **Extended llms.txt** - 15-25 curated URLs covering products/services, documentation, about page, key content
2. **Complete Schema.org** - All required and recommended fields for Product, Service, or Article types
3. **Author attribution** - Person markup for content authors, Organization markup for company pages
4. **Visual content** - Image markup in structured data, proper alt text on all images
5. **Reviews and ratings** - If you have reviews, mark them up with Review or AggregateRating
6. **Breadcrumb navigation** - Structured breadcrumb markup to show content hierarchy
7. **FAQ markup** - For pages answering common questions

**Success criteria:** Agents cite you alongside competitors for relevant queries. Citation accuracy is high (correct facts, current prices, appropriate context).

**Time to implement:** 1-2 weeks for experienced developers, 3-4 weeks for teams learning structured data.

### Phase 3 - Advanced GEO

**Goal:** Differentiate through superior agent compatibility and structured data completeness.

**Requirements:**

1. **Comprehensive llms.txt** - Multi-language versions, regular updates, metadata sections
2. **Advanced Schema.org types** - HowTo markup for tutorials, VideoObject for videos, QAPage for Q&A content, Event markup for webinars/conferences
3. **Dynamic structured data** - Real-time price updates, stock availability, dateModified on content changes
4. **API documentation** - Clear, agent-readable API docs if your service offers programmatic access
5. **Multi-language support** - hreflang tags, language-specific structured data, regional pricing
6. **Agent-specific optimization** - Testing with multiple platforms, iterating based on citation accuracy feedback

**Success criteria:** You're cited more frequently than competitors. Agents reference specific details from your structured data (exact specifications, nuanced comparisons, recent updates).

**Time to implement:** Ongoing, 1-2 hours per week maintaining and improving structured data coverage.

### Measuring Which Phase You're In

**Phase 1 (Minimum Viable):**

- 0-30% of pages have structured data
- llms.txt exists but may be outdated
- Semantic HTML is inconsistent
- You appear in <25% of relevant agent queries

**Phase 2 (Competitive):**

- 30-70% of pages have complete structured data
- llms.txt is current and comprehensive
- Semantic HTML is consistent across main content
- You appear in 25-60% of relevant agent queries

**Phase 3 (Advanced):**

- 70-100% of pages have complete, current structured data
- llms.txt updated quarterly with multi-language support
- Full semantic HTML with advanced markup types
- You appear in 60-90% of relevant agent queries

**How to measure your current phase:**

1. Crawl your site with Screaming Frog or similar tool
2. Calculate percentage of pages with valid JSON-LD
3. Test 20 relevant queries across ChatGPT, Claude, Perplexity
4. Count how many times you're cited
5. Map to the phase descriptions above

This gives you a baseline. Track quarterly to measure progress.

---

## Trust Dividend - Platforms Will Send You Traffic

By designing for the Invisible User, you're future-proofing your business. You're earning what I call the **Trust Dividend.**

In 2026, the platforms - Google, OpenAI, Anthropic, Perplexity - act as gatekeepers. Their agents will only send traffic to sites that have a high success rate for automated tasks.

If agents consistently fail on your site - timeout errors, confusing structure, missing information - platforms will deprioritise you in citations. Why would ChatGPT cite a restaurant it can't extract menu information from? Why would Claude reference a product page where the price is ambiguous?

But if agents consistently succeed - accurate extraction, clear transaction paths, reliable structured data - platforms will trust you. They'll cite you more frequently. They'll send users to you with confidence.

**This compounds.** Every successful agent interaction makes future citations more likely. Every failed interaction makes future citations less likely.

The convergence principle ensures your investment pays multiple dividends:

- **Human accessibility** improves (legal compliance, broader audience reach)
- **SEO performance** improves (better rankings, featured snippets, rich results)
- **Agent citations** increase (GEO visibility, transaction capture)
- **Platform trust** accumulates (compounding discoverability advantage)

**The Invisible User isn't a threat. They're your most efficient customer - if you make sure they can find the door.**

## Real-World Validation - Tailwind Labs Example

Appendix J documents a significant case study that validates the GEO thesis: Tailwind Labs' revenue decline from agents generating free alternatives.

In 2023-2024, Tailwind Labs generated £19 million in revenue from Tailwind UI, a paid component library. By late 2024, revenue had dropped substantially.

**What happened:** AI coding assistants (ChatGPT, Claude, Cursor, GitHub Copilot) learned to generate Tailwind CSS components from scratch. When developers asked for "a responsive navbar with dropdown menus using Tailwind", agents generated working code for free rather than citing Tailwind UI's paid templates.

**The discovery gap:** Tailwind Labs had no llms.txt file directing agents to the paid product. Their documentation focused on the open-source Tailwind CSS framework, not the commercial Tailwind UI offering. Agents trained on public documentation naturally promoted free alternatives.

**The GEO lesson:** Absence of discovery guidance meant agents didn't know the commercial product existed. With proper llms.txt, structured data emphasising the paid tier, and clear differentiation between free framework and premium components, agents could have cited both - giving developers the choice whilst directing commercial intent toward the paid product.

This isn't hypothetical. This happened to a sophisticated technical company with excellent SEO fundamentals. They had complete documentation, semantic HTML, clear information architecture. But they lacked agent-specific discovery guidance, and that gap cost them tens of millions in revenue.

**The broader implication:** If Tailwind Labs - a company at the cutting edge of web development - missed the GEO requirement, most businesses are similarly exposed. The patterns in this chapter aren't optional. They're defensive requirements for maintaining commercial visibility as discovery shifts to AI platforms.

## The Workaround Trap - Why Edge Optimization Might Miss the Point

Tools like Adobe's Optimize at Edge offer a tempting shortcut: serve different content to AI agents than to humans. Pre-render JavaScript-heavy pages. Simplify complex paragraphs. Add summaries. Improve heading structures. All at the CDN layer, without touching your CMS or affecting human visitors.

It's effective. Deploy fixes in minutes instead of weeks. No engineering tickets. No development cycles. Marketing teams can make AI agents see better content without waiting for developers.

**But here's the critical question:** If your content needs simpler paragraphs, clearer headings, and better summaries for AI agents, doesn't it need those things for humans too?

### What Edge Optimization Actually Solves

Edge optimization addresses two genuine problems:

**1. JavaScript visibility** - Modern websites load content through JavaScript after the initial page renders. Browsers execute JavaScript and show users the complete page. AI agents request HTML, receive the initial response, and stop. Product details that load dynamically become invisible. Pre-rendering captures pages after JavaScript executes and serves that to AI agents.

**2. Content optimisation shortcuts** - Simplify complex text. Add page summaries. Improve heading hierarchies. Generate FAQs. These changes only go to AI agents. Humans see the original content.

The first problem is technical. The second is strategic.

### The Opportunity Cost

Creating AI-specific versions of your content is a workaround. It treats symptoms rather than causes.

If your heading structure confuses AI agents, it probably confuses some human readers too. People scanning for information rely on clear heading hierarchies just like language models do. A messy heading structure makes content harder to navigate whether you're human or machine.

If your paragraphs are too complex for AI to process effectively, they might be too complex for some readers as well. Content readability affects comprehension across the board. Simplifying text helps everyone, not just AI agents.

If your pages need summaries to help AI understand what they're about, those summaries would help human readers too. Particularly people scanning search results, accessing content on mobile devices, or trying to determine if a long article is worth reading.

**Edge optimization lets you improve how AI sees your content without improving the content itself.** That's useful when you're under time pressure or can't modify your CMS. But it creates a divergence - two versions of the same page, one optimised and one not.

The better approach: fix the underlying content. Make headings clearer for everyone. Write more readable paragraphs. Add useful summaries. Structure information better. These changes benefit humans and AI agents simultaneously.

### When Edge Optimization Makes Sense

Pre-rendering addresses a genuine technical constraint. AI agents fundamentally cannot execute JavaScript the way browsers do. If rebuilding your site architecture to support server-side rendering isn't feasible, pre-rendering at the edge solves the visibility problem without requiring a complete platform overhaul.

Edge optimisation also makes sense for:

- **Legacy systems** - Your CMS might not support the heading hierarchy you want. Your templates might not allow for page summaries. Your publishing workflow might make content improvements impractical.
- **Time-sensitive situations** - If your content is invisible to AI agents right now and fixing it properly requires months of work, serving pre-rendered pages immediately whilst planning longer-term improvements makes practical sense.
- **Platform limitations** - Working around constraints you cannot change in reasonable timeframes.

### The Technical vs Strategic Choice

Edge optimization represents a technical solution to what might be a strategic problem. Your content structure, information architecture, and publishing workflow created the gaps that edge optimization now fills.

Technical solutions work. They're often faster and cheaper than strategic changes. Deploy a CDN rule instead of restructuring your content model. Serve pre-rendered HTML instead of rebuilding your application architecture. Generate summaries automatically instead of training content teams to write better.

**But technical solutions don't compound the same way strategic ones do.** Edge optimizations help AI agents see your content better. Improving your actual content helps everyone who reads it, now and in the future, regardless of how they access it.

### The Diagnostic Value

The tool's real value might be diagnostic. It shows you what's wrong with your content from a machine-readability perspective. Use those insights to improve content quality broadly rather than just patching what AI agents see.

Simplifying complex paragraphs at the edge? Good short-term fix. Better long-term fix - write clearer paragraphs in your CMS so everyone benefits.

Improving heading hierarchies for AI agents only? Useful workaround. Better approach - restructure headings properly so both humans and AI get clear navigation.

Adding AI-specific summaries? Helps with discoverability. More effective - add summaries to the actual page so readers and search engines see them too.

**The question isn't whether edge optimization works - it does. The question is whether it's the right long-term approach for your content strategy, or whether it's a workaround that lets you avoid addressing fundamental content quality issues.**

This connects directly to the convergence principle: patterns that help AI agents also help humans. Edge optimization lets you bypass that convergence, serving different content to different visitors. Sometimes that's necessary. But it shouldn't be the goal.

---

## Connecting to Chapter 11 Implementation

This chapter established the business case for GEO and the strategic framework for when and how to optimise for agent discovery.

The next chapter, "Designing for Both", provides the technical implementation patterns that make GEO practical. Where this chapter explained why structured data matters for discovery, Chapter 11 shows you how to implement that structured data so it serves both discovery (GEO) and usability (agent transactions).

The convergence continues: discovery patterns naturally lead to usability patterns. When agents can find you accurately, the next question is whether they can complete tasks successfully once they arrive.

Chapter 11 answers that question.
